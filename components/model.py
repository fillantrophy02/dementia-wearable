import math
import torch
import torch.nn as nn
import torchmetrics
from components.model_wearable_korean import WearableKoreanSleepPatchTST
from config import *
from transformers import PatchTSTConfig, PatchTSTModel

class SleepPatchTST(nn.Module):
    '''Expect input of size (N, L, H_in) where N = batch_size, L = length of sequence, H_in = input_size aka no. of features'''
    def __init__(self, input_size):
        super().__init__()

        if "TL" in special_mode_suffix:
            if "Wearable_Korean" in special_mode_suffix:
                self.reshape1 = nn.Linear(input_size, 10)
                pretrained =  WearableKoreanSleepPatchTST()
                pretrained.load_state_dict(torch.load('ckpts/Wearable_Korean.pth'))
                self.backbone = pretrained.backbone
                self.hidden1 = nn.Linear(10 * hidden_size, 64)
                
        else:
            config = PatchTSTConfig(
                num_input_channels=input_size,
                context_length=no_of_days,
                patch_length=2,
                patch_stride=1,
                num_hidden_layers=num_layers,
                d_model=hidden_size,
                attention_dropout=dropout,
                ff_dropout=dropout,
                ffn_dim=ffn_dim,
                scaling=None,
                prediction_length=prediction_length,
                num_targets=input_size
            )
            self.backbone = PatchTSTModel(config).to(device)
            self.hidden1 = nn.Linear(input_size * hidden_size, 64)

        self.fc = nn.Linear(64, 1)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.ELU()
        self.sigmoid = nn.Sigmoid()
        self.loss = nn.BCELoss()

    def forward(self, x):
        # if "TL" in special_mode_suffix:
        #     if "Wearable_Korean" in special_mode_suffix:
        #         x = self.dropout(self.activation(self.reshape1(x)))

        outputs = self.backbone(
            past_values=x, # past mask & pos encoding will be auto generated by backbone
        )
        h = outputs.last_hidden_state # (N, input_size, num_patches, hidden_size)
        h = h.mean(dim=2) # (N, input_size, hidden_size)
        h = torch.flatten(h, 1) # (N, input_size * hidden_size)
        x = self.dropout(self.activation(self.hidden1(h)))
        x = self.sigmoid(self.fc(x))
        return x