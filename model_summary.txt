SleepPatchTST(
  (reshape1): Linear(in_features=26, out_features=32, bias=True)
  (backbone): PatchTSTModel(
    (scaler): PatchTSTScaler(
      (scaler): PatchTSTNOPScaler()
    )
    (patchifier): PatchTSTPatchify()
    (masking): Identity()
    (encoder): PatchTSTEncoder(
      (embedder): PatchTSTEmbedding(
        (input_embedding): Linear(in_features=2, out_features=64, bias=True)
      )
      (positional_encoder): PatchTSTPositionalEncoding(
        (positional_dropout): Identity()
      )
      (layers): ModuleList(
        (0-2): 3 x PatchTSTEncoderLayer(
          (self_attn): PatchTSTAttention(
            (k_proj): Linear(in_features=64, out_features=64, bias=True)
            (v_proj): Linear(in_features=64, out_features=64, bias=True)
            (q_proj): Linear(in_features=64, out_features=64, bias=True)
            (out_proj): Linear(in_features=64, out_features=64, bias=True)
          )
          (dropout_path1): Identity()
          (norm_sublayer1): PatchTSTBatchNorm(
            (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (ff): Sequential(
            (0): Linear(in_features=64, out_features=512, bias=True)
            (1): GELUActivation()
            (2): Dropout(p=0.5, inplace=False)
            (3): Linear(in_features=512, out_features=64, bias=True)
          )
          (dropout_path3): Identity()
          (norm_sublayer3): PatchTSTBatchNorm(
            (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (hidden1): Linear(in_features=2048, out_features=64, bias=True)
  (fc): Linear(in_features=64, out_features=1, bias=True)
  (dropout): Dropout(p=0.5, inplace=False)
  (activation): ELU(alpha=1.0)
  (sigmoid): Sigmoid()
  (loss): BCELoss()
)