import math
import torch
import torch.nn as nn
import torchmetrics
from config import *
from pretrained_for_TL.wearable_korean import config as pretrained_config
from transformers import PatchTSTConfig, PatchTSTModel

class FitbitMCIPatchTST(nn.Module):
    '''Expect input of size (N, L, H_in) where N = batch_size, L = length of sequence, H_in = input_size aka no. of features'''
    def __init__(self):
        super().__init__()

        config = PatchTSTConfig(
                num_input_channels=pretrained_config.input_size,
                context_length=pretrained_config.no_of_days,
                patch_length=pretrained_config.patch_length,
                patch_stride=pretrained_config.patch_stride,
                num_hidden_layers=pretrained_config.num_layers,
                d_model=pretrained_config.hidden_size,
                attention_dropout=pretrained_config.dropout,
                ff_dropout=pretrained_config.dropout,
                ffn_dim=pretrained_config.ffn_dim,
                prediction_length=pretrained_config.prediction_length,
                num_targets=pretrained_config.input_size
            )
        self.backbone = PatchTSTModel(config).to(device)
        self.hidden1 = nn.Linear(pretrained_config.input_size * pretrained_config.hidden_size, pretrained_config.hidden_size)

        self.fc = nn.Linear(pretrained_config.hidden_size, 1)
        self.dropout = nn.Dropout(pretrained_config.dropout)
        self.activation = nn.ELU()
        self.sigmoid = nn.Sigmoid()
        self.loss = nn.BCELoss()

    def forward(self, x):
        outputs = self.backbone(
            past_values=x, # past mask & pos encoding will be auto generated by backbone
        )
        h = outputs.last_hidden_state # (N, input_size, num_patches, hidden_size)
        h = h.mean(dim=2) # (N, input_size, hidden_size)
        h = torch.flatten(h, 1) # (N, input_size * hidden_size)
        x = self.dropout(self.activation(self.hidden1(h)))
        x = self.sigmoid(self.fc(x))
        return x